{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the Pet Image Dataset\n",
    "path = untar_data(URLs.PETS)\n",
    "\n",
    "# Setting the base path to the pet images directory we have just downloaded\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "# Using the data block API\n",
    "pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                 get_items = get_image_files,\n",
    "                 splitter = RandomSplitter(seed=42),\n",
    "                 get_y = using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "                 item_tfms = Resize(460),\n",
    "                 batch_tfms = aug_transforms(size=224, min_scale=0.75))\n",
    "\n",
    "# Creating a DataLoaders object\n",
    "dls = pets.dataloaders(path/'images')\n",
    "\n",
    "# Creating a simple model\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs and Cats to Pet Breeds\n",
    "\n",
    "## Cross Entropy Loss\n",
    "Remember that *loss* is whatever function we've decided to use to optimise the parameters of our model. But we haven't actually told fastai what loss function we want to us. \n",
    "\n",
    "fastai will generally try to select an appropriate loss function based on what kind of data and model you are using. In this case, we have image data and a categorical outcome, so fastai will default to using cross-entropy loss.\n",
    "\n",
    "*Cross-entropy loss* is a loss function that is similar to the one we used in the previous chapter, but has two benefits:\n",
    "- It works even when our dependent variable has more than two categories\n",
    "- It results in faster and more reliable training\n",
    "\n",
    "To understand how cross-entropy loss works for dependent variables with more than two categories, we have to understand what the actual data and activations that are seen by the loss function look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Activations and Labels\n",
    "Let's have a look at the activations of our model. To get a batch of real data from our `DataLoaders`, we can use the `one_batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the dependent and independent variables as a mini-batch. Let's see what is actually contained in our dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our batch size is 64, so we have 64 rows in this tensor. Each row is a single integer between 0 and 36, represeneting our 37 possible pet breeds.\n",
    "\n",
    "We can view the predictions (activations of the final layer of our neural network) using `Learner.get_preds`. This function either takes a dataset index (0 for train and 1 for valid) or an iterator of batches. Thus, we can pass in a simple list with our batch to get our predictions.\n",
    "\n",
    "By default, it returns predictions and targets, but since we already have the targets, we can effectively ignore them by assigning to the special variable `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,_ = learn.get_preds(dl=[(x,y)])\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual predictions are 37 probabilities between 0 and 1, which add up to 1 in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds[0]), preds[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the activations of our model into predictions like this, we used something called the *softmax* activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "In our classification model, we use the softmax activation function in the final layer to ensure that the activations are all between 0 and 1, and that they sum to 1. It is similar to the sigmoid function we saw earlier.\n",
    "\n",
    "To think about what happens if we want to have more categories in our target (such as our 37 pet breeds), we'll need more activations than just a single column: we need an activation *per category*. We can therefore create, for instance, a neural net that predicts 3s and 7s that returns two activations, one for each class. This will be a good step towards creating the the more general approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off using some random numbers with a standard deviation of 2 for our example. Assuming we have 6 images and 2 possible categories (where the first column represents 3s and the second is 7s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42);\n",
    "acts = torch.randn((6,2))*2\n",
    "acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just take the sigmoid of this directly, since we don't get rows that add to 1. (i.e., We want the probability of being a 3 plus the probability of being a 7 add up to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our neural net created a single activation per image, which we passed through the `sigmoid` function. That single activation represented the model's confidence that the input was a 3. Binary problems are a special case of classification problems, because the target can get treated as a single boolean value, as we did in `mnist_loss`. But binary problems can also be thought of in the context of the more general group of classifiers with any number of categories: in this case, we happen to have two categories. As we saw in the bear classifier, our neural net will return one activation per category.\n",
    "\n",
    "In the binary case, what do those activations really indicate? A single pair of activations simply indicates the *relative confidence* of the input being a 3 verus being a 7. The overall values, whether they are both high, or both low, don't matter - all that matters is which is higher and by how much.\n",
    "\n",
    "We would expect that since this is just another way of representing the same problem, that we would be able to use `sigmoid` directly on the two-activation version of our neural net. Indeed we can!\n",
    "We just take the *difference* between the two neural net activations, because that reflects how much more sure we are of the input being a 3 than a 7, and then take the sigmoid of that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(acts[:,0] - acts[:,1]).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second column (the probability of it being a 7) will then just be that value subtracted from 1.\n",
    "\n",
    "Now, we need a way to do all this that also works for more than two columns. The softmax function does exactly that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that `softmax` returns the same values as `sigmoid` for the first column, and those values subtracted from 1 for the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_acts = torch.softmax(acts, dim=1)\n",
    "sm_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` is the multi-category equivalent of `sigmoid` - we have to use it any time we have more than two categories and the probabilities of the categories must add to 1. We often use it even for just two categories, just to make things more consistent. We'll also see shortly that the softmax function works hand-in-hand with the loss function we will look at in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood\n",
    "Softmax is the first part of cross-entropy loss - the second part is log likelihood.\n",
    "\n",
    "When we calculated the loss for our MNIST example in the last chapter, we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(inputs, targets):\n",
    "    inputs = inputs.sigmoid()\n",
    "    return torch.where(targets==1, 1-inputs, inputs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we moved from sigmoid to softmax, we need to extend our loss function to work with more than just binary classification. It needs to be able to classify any number of categories (in this case, we have 37 categories).\n",
    "\n",
    "Our activations, after softmax, are between 0 and 1, and sum to 1 for each row in the batch of predictions. Our targets are integers between 0 and 36. Furthermore, cross-entropy loss generalises our binary classification loss and allows for more than one correct label per example (which is called multi-label classification).\n",
    "\n",
    "In the binary case, we used `torch.where` to select between `inputs` and `1-inputs`. When we treat a binary classification problem as a general classification problem with two categories, it actually becomes even easier, because we now have two columns containing the equivalent of `inputs` and `1-inputs`. As there is only one correct label per example, all we need to do is select the appropriate column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement this in PyTorch. For our synthetic 3s and 7s example, let's say the following are our labels and our softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = tensor([0,1,0,1,1,0])\n",
    "\n",
    "sm_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each item of `targ`, we can use that to select the appropriate column of `sm_acts` using tensor indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(6)\n",
    "sm_acts[idx, targ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's exactly happening here, let's put all the columns together in a table. Here, the first two columns are our activations, then we have the targets and the row index. We explain the last column, `result` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "df = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\n",
    "df['targ'] = targ\n",
    "df['idx'] = idx\n",
    "df['result'] = sm_acts[range(6), targ]\n",
    "t = df.style.hide_index()\n",
    "#To have html code compatible with our script\n",
    "html = t._repr_html_().split('</style>')[1]\n",
    "html = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table, we can see that the `result` column is what `sm_acts[idx, targ]` was doing. The really interesting thing is that it works just as well with more than two columns. To see this, consider that would happen if we added an activation column for every digit (0 through 9), and then `targ` contained a number from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a function that does exactly the same as `sm_acts[range(n), targ]` (exept it takes the negative, because when applying the log afterward, we will have negative numbers), called `nll_loss` (*NLL* stands for *negative log likelihood*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-sm_acts[idx, targ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.nll_loss(sm_acts, targ, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, this PyTorch function does not take the log. It assumes we have *already* taken the log.\n",
    "\n",
    "PyTorch has a function called `log_softmax` that combines `log` and `softmax` in a fast and accurate way. `nll_loss` is designed to be used after `log_softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we take the Log?\n",
    "Cross entropy loss may involve the multiplication of many numbers. Multiplying lots of negative numbers together can call problems like *numerical underflow* in computers. We therefore want to transform these probabilities to larger values so we can perform mathematical operations on them.\n",
    "\n",
    "The *logarithm* function does exactly this (available as `torch.log`).\n",
    "\n",
    "Additionally, we want to ensure our model is able to detect differences between small numbers. For example, consider the probabilities of .01 and .001. Although these numbers are very close together, 0.01 is 10 times more confident than 0.001. By taking the log of our probabilities, we prevent these important differences from being ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key thing to know about logarithms is the following relationship.\n",
    "```\n",
    "    log(a*b) = log(a) + log(b)\n",
    "```\n",
    "\n",
    "It means that logarithms increase linearly when the underlying signal increases exponentially or multiplicatively. We love using logarithms, because it means that multiplication, which can create really large and really small numbers, can be replaced by addition, which is much less likely to result in scales that are difficult for our computers to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the log of a number approaches negative infinity as the number approaches zero. In our case, since the result reflects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is \"good\" (closer to 1) and a large value when the prediction is \"bad\" (closer to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(lambda x: -1*torch.log(x), min=0,max=1, tx='x', ty='- log(x)', title = 'Log Loss when true label = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go ahead and update our previous results table with an additional column, `loss` to reflect this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "df['loss'] = -torch.log(tensor(df['result']))\n",
    "t = df.style.hide_index()\n",
    "#To have html code compatible with our script\n",
    "html = t._repr_html_().split('</style>')[1]\n",
    "html = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the loss is very large in the 3rd and 4th rows, where the predictions are confident and wrong. A benefit of using the log to calculate the loss is that our loss function penalises predictions that are both confident and wrong. This kind of penalty works well in practice to aid in more effective model training.\n",
    "\n",
    "We are calculating the loss from the column containing the correct label. Because there is only one \"right\" answer per example, we don't need to consider the other columns, because by the definition of softmax, they add up to 1 minus the activation corresponding to the correct label. As long as the activation columns sum to 1, then we'll have a loss function that shows how well we're predicting each digit. Therefore, making the activation for the correct label as high as possible must mean we're also decreasing the activations of the remaining columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood\n",
    "\n",
    "Taking the mean of the negative log of our probabilities (i.e., taking the mean of the `loss` column of our table) gives use the *negative log likelihood loss*, which is just another name for cross-entropy loss. Recall that PyTorch's `nll_loss` assumes that you already took the log of the softmax, so it doesn't actually do the logarithm for you.\n",
    "\n",
    "In PyTorch, this is available as `nn.CrossEntropyLoss` (which, in practice, actually does `log_softmax` and then `nll_loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a class. Instantiating it gives you an object which behaves like a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(acts, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All PyTorch loss functions are provided in two forms, the class shown above or the functional form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(acts, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, PyTorch loss functions take the mean of the loss of all items. You can use `reduction='none'` to disable that.\n",
    "The values should make the `loss` column in our table exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss(reduction='none')(acts, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen all the pieces hidden behind our loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e90582675576e512e92beafc5221de7144a1dea5f49197901594ad708e1d2ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
