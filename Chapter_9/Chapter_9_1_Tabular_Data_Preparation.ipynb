{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from dtreeviz.trees import *\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Modelling Deep Dive\n",
    "\n",
    "Tabular modelling takes data in the form of a table (like a spreadsheet or CSV). The objective is to predict the value in one column based on the values in the other columns. In this chapter, we will not only look at deep learning but also more general machine learning techniques like random forests, as they can give better results depending on your problem.\n",
    "\n",
    "We will look at how we should pre-process and clean the data, as well as how to interpret the result of our models after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Embeddings\n",
    "\n",
    "In tabular data, some columns/variables may be *continuous* whilst others may be *categorical*. Continuous variables are numerical data and can be directly fed to a model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as \"movie ID\", for which addition and multiplication don't have meaning.\n",
    "\n",
    "The paper \"Entity Embeddings for Categorical Variables\" demonstrated one of the earliest known examples of a state-of-the-art deep learning tabular model. Entity embeddings are used to map similar values close to each other in the embedding space, revealing the intrinsic properties of categorical variables. They are used to transform categorical variables into inputs that are both continuous and meaningful. This is achieved by defining a distance measure for our categorical variables. It is especially useful for datasets with lots of high cardinality features and can be used for visualising categorical data and for data clustering.\n",
    "\n",
    "The main insight from entity embeddings is that we can provide a model with fundamentally categorical data about discrete entities and that the model learns an embedding for these entities that defines a continuous notion of distance between them. Because the embedding distance was learned based on real patterns in the data, the distance tends to match up with our intuitions. In addition, it is valuable in its own right that embeddings are continuous, because models are better at understanding continuous variables. This is unsurprising considering that models are built of many continuous parameter weights and continuous activation values, which are updated via gradient descent.\n",
    "\n",
    "Another benefit is that we can combine our continuous embedding values with truly continuous input data in a straightforward manner; we just concatenate the variables and feed the concatenation into our first linear layer. In other words, the raw categorical data is transformed by an embedding layer before it interacts with the raw continuous input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning vs. Deep Learning for Structured Data\n",
    "\n",
    "Most modern machine learning can be distilled down to a few key techniques that are widely applicable. The vast majority of datasets can be best modelled with just two methods:\n",
    "1. Ensembles of decision trees, mainly for structured data.\n",
    "2. Multilayered neural networks learned with SGD, mainly for unstructured data.\n",
    "\n",
    "Although deep learning is nearly always superior for unstructured data, these two approaches tend to give similar results for many kinds of structured data. But ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning.\n",
    "\n",
    "Most importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles. There are tools and methods for answering the pertinent questions like:\n",
    "- Which columns in the dataset were the most important for your predictions?\n",
    "- How are they related to the dependent variable?\n",
    "- How do they interact with each other?\n",
    "- Which particular features were most important for some particular observation?\n",
    "Therefore, ensembles of decision trees should be our first approach for analysing a new tabular dataset.\n",
    "\n",
    "The exception to this guideline is when the dataset meets one of these conditions:\n",
    "- There are some high-cardinality categorical variables that are very important (high number of discrete levels representing categories e.g. Postcode).\n",
    "- There are some columns that contain data that would be best understood with a neural network, such as plain text data.\n",
    "\n",
    "In practice, when we deal with datasets that meet these exceptional conditions, we always try both decision tree ensembles and deep learning to see which works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "The dataset we use in this chapter is from the Blue Book for Bulldozers Kaggle competition, which has the following description: \"The goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n",
    "\n",
    "This is a very common type of dataset and prediction problem, similar to what you may see in your project or workplace. The dataset is available for download on Kaggle, a website that hosts data science compitions.\n",
    "\n",
    "The easiest way to download Kaggle datasets is to use the Kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle API Token\n",
    "creds = '{\"username\": \"hobz462\", \"key\": \"286dd5e15f0963c88007cd89dee68a56\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving API Token locally\n",
    "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying a path to download the dataset to\n",
    "comp = 'bluebook-for-bulldozers'\n",
    "path = URLs.path(comp)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) [Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/random_forest_benchmark_test.csv'),Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/Valid.csv'),Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/median_benchmark.csv'),Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/Test.csv'),Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/ValidSolution.csv'),Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/Machine_Appendix.csv'),Path('/Users/286329i/.fastai/archive/bluebook-for-bulldozers/TrainAndValid.csv')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Kaggle API to download the dataset to that path and extracting it\n",
    "from kaggle import api\n",
    "\n",
    "if not path.exists():\n",
    "    path.mkdir(parents=true)\n",
    "    api.competition_download_cli(comp, path=path)\n",
    "    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n",
    "\n",
    "path.ls(file_type='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Data\n",
    "Kaggle provides information about some of the fields of our dataset. The [Data](https://www.kaggle.com/c/bluebook-for-bulldozers/data) explains that the key fields in *train.csv* are:\n",
    "\n",
    "- `SalesID`:: The unique identifier of the sale.\n",
    "- `MachineID`:: The unique identifier of a machine.  A machine can be sold multiple times.\n",
    "- `saleprice`:: What the machine sold for at auction (only provided in *train.csv*).\n",
    "- `saledate`:: The date of the sale.\n",
    "\n",
    "In any sort of data science work, it's important to *look at your data directly* to make sure you understand the format, how it's stored, what types of values it holds, etc. Even if you've read a description of the data, the actual data may not be what you expect. We'll start by reading the training set into a Pandas DataFrame. Generally it's a good idea to specify `low_memory=False` unless Pandas actually runs out of memory and returns an error. The `low_memory` parameter, which is `True` by default, tells Pandas to only look at a few rows of data at a time to figure out what type of data is in each column. This means that Pandas can actually end up using different data type for different rows, which generally leads to data processing errors or model training problems later.\n",
    "\n",
    "Let's load our data and have a look at the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n",
       "       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
       "       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n",
       "       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n",
       "       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n",
       "       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n",
       "       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n",
       "       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n",
       "       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n",
       "       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n",
       "       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n",
       "       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n",
       "       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of columns for us to look at! Try looking through the dataset to get a sense of what kind of information is in each one. We'll shortly see how to \"zero in\" on the most interesting bits.\n",
    "\n",
    "At this point, a good next step is to handle *ordinal columns*. This refers to columns containing strings or similar, but where those strings have a natural ordering. For instance, here are the levels of `ProductSize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ProductSize'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell Pandas about a suitable ordering of these levels like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            NaN\n",
       "1         Medium\n",
       "2            NaN\n",
       "3          Small\n",
       "4            NaN\n",
       "           ...  \n",
       "412693      Mini\n",
       "412694      Mini\n",
       "412695      Mini\n",
       "412696      Mini\n",
       "412697      Mini\n",
       "Name: ProductSize, Length: 412698, dtype: category\n",
       "Categories (6, object): ['Large' < 'Large / Medium' < 'Medium' < 'Small' < 'Mini' < 'Compact']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting 'ProductSize' into an ordinal column\n",
    "df['ProductSize'] = df['ProductSize'].astype('category')\n",
    "df['ProductSize'].cat.set_categories(sizes, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important data column is the dependent variable—that is, the one we want to predict. Recall that a model's metric is a function that reflects how good the predictions are. It's important to note what metric is being used for a project. Generally, selecting the metric is an important part of the project setup. In many cases, choosing a good metric will require more than just selecting a variable that already exists. It is more like a design process. You should think carefully about which metric, or set of metrics, actually measures the notion of model quality that matters to you. If no variable represents that metric, you should see if you can build the metric from the variables that are available.\n",
    "\n",
    "However, in this case Kaggle tells us what metric to use: root mean squared log error (RMSLE) between the actual and predicted auction prices. We need do only a small amount of processing to use this: we take the log of the prices, so that `rmse` of that value will give us what we ultimately need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the log of SalePrice to use the metric RMSLE\n",
    "dep_var = 'SalePrice'\n",
    "df[dep_var] = np.log(df[dep_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Dates\n",
    "The first piece of data representation we need to do is to enrich our representation of dates. You might want to treat a date as an ordinal value, because it is meaningful to say that one date is greater than the other. However, dates are a bit different from most ordinal values in that some dates are qualititatively different from others in a way that is often relevant to the systems we are modelling.\n",
    "\n",
    "In order to help our algorithms handle dates intelligently, we'd like our model to know more than whether a date is more recent or less recent than another. We might want our model to make decisions based on that date's day of the week, on whether a day is a holiday, on what month it is in, and so forth. To do this, we replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful.\n",
    "\n",
    "fastai comes with a function that will do this for us - we just have to pass a column name that contains dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features from 'saledate'\n",
    "df = add_datepart(df, 'saledate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the test set while we're here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path/'Test.csv', low_memory=False)\n",
    "df_test = add_datepart(df_test, 'saledate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are now lots of new columns in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(o for o in df.columns if o.startswith('sale'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good first step, but we will need to do a bit more cleaning. For this, we will use fastai objects called `TabularPandas` and `TabularProc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TabluarPandas and TabularProc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second piece of preparatory processing to be sure we can handle strings and missing data.\n",
    "\n",
    "Out of the box, sklearn canot do either. Instead we will use fastai's class `TabularPandas`, which wraps a Pandas DataFrame and provides a few conveniences. To populate a `TabularPandas`, we will use two `TabularProcs`:\n",
    "1. `Categorify`\n",
    "2. `FillMissing`\n",
    "\n",
    "A `TabularProc` is like a regular `Transform`, except that:\n",
    "- It returns the exact same object that's passed to it, after modifying the object in place.\n",
    "- It runs the transform once, when data is first passed in, rather than lazily as the data is accessed.\n",
    "\n",
    "`Categorify` is a `TabularProc` that replaces a column with a numeric categorical column.\n",
    "\n",
    "`FillMissing` is a `TabularProc` that replaces missing values with the median of the column, and creates a new Boolean column that is set to `True` for any row where the value was missing.\n",
    "\n",
    "These two transforms are needed for nearly every tabular dataset you will use, so this is a good starting point for your data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [Categorify, FillMissing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabularPandas` will also handle splitting the dataset into training and validation sets for us. However, we need to be very careful about our validation set. We want to design it so that it is like the *test* set Kaggle will use to judge the contest.\n",
    "\n",
    "We don't get to see the test set. But we do want to define our validation data so that it has the same sort of relationship to the training data as the test set will have. In some cases, just randomly chosing a subset of your data oints will do that. This is not one of those cases, because it is a time series.\n",
    "\n",
    "If you look at the data range represented in the test set, you will discover that it covers a six-month period from May 2012, which is later in time than any date in the training set. This is a good design, because the competition sponsor will want to ensure that the model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time than the training set. The Kaggle training data ends in April 2012, so we will define a narrower training dataset which consists only of the Kaggle training data from before November 2011, and we'll define a validation set consisting of data from after November 2011.\n",
    "\n",
    "To do this, we use `np.where`, a useful function that returns (as the first element of a tuple) the indices of all `True` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets where the last 6 months are used for validation\n",
    "cond = (df.saleYear<2011) | (df.saleMonth<10)\n",
    "train_idx = np.where(cond)[0]\n",
    "valid_idx = np.where(~cond)[0]\n",
    "\n",
    "splits = (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabularPandas` needs to be told which columns are continuous and which are categorical. We can handle that automatically using the helper function `cont_cat_split`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying our continuous and categorical columns\n",
    "cont,cat = cont_cat_split(df, 1, dep_var=dep_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our TabularPandas object\n",
    "to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TabularPandas` behaves a lot like a fastai `Datasets` object, including providing `train` and `valid` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404710, 7988)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to.train), len(to.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is still disabled as strings for categories (we only show a full columns here because the full table is too big):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UsageBand</th>\n",
       "      <th>fiModelDesc</th>\n",
       "      <th>fiBaseModel</th>\n",
       "      <th>fiSecondaryDesc</th>\n",
       "      <th>fiModelSeries</th>\n",
       "      <th>fiModelDescriptor</th>\n",
       "      <th>ProductSize</th>\n",
       "      <th>fiProductClassDesc</th>\n",
       "      <th>state</th>\n",
       "      <th>ProductGroup</th>\n",
       "      <th>ProductGroupDesc</th>\n",
       "      <th>Drive_System</th>\n",
       "      <th>Enclosure</th>\n",
       "      <th>Forks</th>\n",
       "      <th>Pad_Type</th>\n",
       "      <th>Ride_Control</th>\n",
       "      <th>Stick</th>\n",
       "      <th>Transmission</th>\n",
       "      <th>Turbocharged</th>\n",
       "      <th>Blade_Extension</th>\n",
       "      <th>Blade_Width</th>\n",
       "      <th>Enclosure_Type</th>\n",
       "      <th>Engine_Horsepower</th>\n",
       "      <th>Hydraulics</th>\n",
       "      <th>Pushblock</th>\n",
       "      <th>Ripper</th>\n",
       "      <th>Scarifier</th>\n",
       "      <th>Tip_Control</th>\n",
       "      <th>Tire_Size</th>\n",
       "      <th>Coupler</th>\n",
       "      <th>Coupler_System</th>\n",
       "      <th>Grouser_Tracks</th>\n",
       "      <th>Hydraulics_Flow</th>\n",
       "      <th>Track_Type</th>\n",
       "      <th>Undercarriage_Pad_Width</th>\n",
       "      <th>Stick_Length</th>\n",
       "      <th>Thumb</th>\n",
       "      <th>Pattern_Changer</th>\n",
       "      <th>Grouser_Type</th>\n",
       "      <th>Backhoe_Mounting</th>\n",
       "      <th>Blade_Type</th>\n",
       "      <th>Travel_Controls</th>\n",
       "      <th>Differential_Type</th>\n",
       "      <th>Steering_Controls</th>\n",
       "      <th>saleIs_month_end</th>\n",
       "      <th>saleIs_month_start</th>\n",
       "      <th>saleIs_quarter_end</th>\n",
       "      <th>saleIs_quarter_start</th>\n",
       "      <th>saleIs_year_end</th>\n",
       "      <th>saleIs_year_start</th>\n",
       "      <th>auctioneerID_na</th>\n",
       "      <th>MachineHoursCurrentMeter_na</th>\n",
       "      <th>SalesID</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>datasource</th>\n",
       "      <th>auctioneerID</th>\n",
       "      <th>YearMade</th>\n",
       "      <th>MachineHoursCurrentMeter</th>\n",
       "      <th>saleYear</th>\n",
       "      <th>saleMonth</th>\n",
       "      <th>saleWeek</th>\n",
       "      <th>saleDay</th>\n",
       "      <th>saleDayofweek</th>\n",
       "      <th>saleDayofyear</th>\n",
       "      <th>saleElapsed</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low</td>\n",
       "      <td>521D</td>\n",
       "      <td>521</td>\n",
       "      <td>D</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Wheel Loader - 110.0 to 120.0 Horsepower</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>WL</td>\n",
       "      <td>Wheel Loader</td>\n",
       "      <td>#na#</td>\n",
       "      <td>EROPS w AC</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>2 Valve</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1139246</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>320</td>\n",
       "      <td>1.163635e+09</td>\n",
       "      <td>11.097410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Low</td>\n",
       "      <td>950FII</td>\n",
       "      <td>950</td>\n",
       "      <td>F</td>\n",
       "      <td>II</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Wheel Loader - 150.0 to 175.0 Horsepower</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>WL</td>\n",
       "      <td>Wheel Loader</td>\n",
       "      <td>#na#</td>\n",
       "      <td>EROPS w AC</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>2 Valve</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>23.5</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1139248</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>4640.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>1.080259e+09</td>\n",
       "      <td>10.950807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High</td>\n",
       "      <td>226</td>\n",
       "      <td>226</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity</td>\n",
       "      <td>New York</td>\n",
       "      <td>SSL</td>\n",
       "      <td>Skid Steer Loaders</td>\n",
       "      <td>#na#</td>\n",
       "      <td>OROPS</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Auxiliary</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>Standard</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1139249</td>\n",
       "      <td>434808</td>\n",
       "      <td>7009</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>2838.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>1.077754e+09</td>\n",
       "      <td>9.210340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High</td>\n",
       "      <td>PC120-6E</td>\n",
       "      <td>PC120</td>\n",
       "      <td>#na#</td>\n",
       "      <td>-6E</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Small</td>\n",
       "      <td>Hydraulic Excavator, Track - 12.0 to 14.0 Metric Tons</td>\n",
       "      <td>Texas</td>\n",
       "      <td>TEX</td>\n",
       "      <td>Track Excavators</td>\n",
       "      <td>#na#</td>\n",
       "      <td>EROPS w AC</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>2 Valve</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1139251</td>\n",
       "      <td>1026470</td>\n",
       "      <td>332</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>1.305763e+09</td>\n",
       "      <td>10.558414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Medium</td>\n",
       "      <td>S175</td>\n",
       "      <td>S175</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Skid Steer Loader - 1601.0 to 1751.0 Lb Operating Capacity</td>\n",
       "      <td>New York</td>\n",
       "      <td>SSL</td>\n",
       "      <td>Skid Steer Loaders</td>\n",
       "      <td>#na#</td>\n",
       "      <td>EROPS</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>Auxiliary</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>None or Unspecified</td>\n",
       "      <td>Standard</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>#na#</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1139253</td>\n",
       "      <td>1057373</td>\n",
       "      <td>17311</td>\n",
       "      <td>121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>722.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>204</td>\n",
       "      <td>1.248307e+09</td>\n",
       "      <td>9.305651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the underlying items are all numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesID</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>...</th>\n",
       "      <th>saleIs_year_start</th>\n",
       "      <th>saleElapsed</th>\n",
       "      <th>auctioneerID_na</th>\n",
       "      <th>MachineHoursCurrentMeter_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139246</td>\n",
       "      <td>11.097410</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.163635e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139248</td>\n",
       "      <td>10.950807</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.080259e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139249</td>\n",
       "      <td>9.210340</td>\n",
       "      <td>434808</td>\n",
       "      <td>7009</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.077754e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1139251</td>\n",
       "      <td>10.558414</td>\n",
       "      <td>1026470</td>\n",
       "      <td>332</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.305763e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1139253</td>\n",
       "      <td>9.305651</td>\n",
       "      <td>1057373</td>\n",
       "      <td>17311</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.248307e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesID  SalePrice  MachineID  ModelID  ...  saleIs_year_start  \\\n",
       "0  1139246  11.097410     999089     3157  ...                  1   \n",
       "1  1139248  10.950807     117657       77  ...                  1   \n",
       "2  1139249   9.210340     434808     7009  ...                  1   \n",
       "3  1139251  10.558414    1026470      332  ...                  1   \n",
       "4  1139253   9.305651    1057373    17311  ...                  1   \n",
       "\n",
       "    saleElapsed  auctioneerID_na  MachineHoursCurrentMeter_na  \n",
       "0  1.163635e+09                1                            1  \n",
       "1  1.080259e+09                1                            1  \n",
       "2  1.077754e+09                1                            1  \n",
       "3  1.305763e+09                1                            1  \n",
       "4  1.248307e+09                1                            1  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.items.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column, so there's no particular meaning to the numbers in categorical columns after conversion. The exception is if you first convert a column to a Pandas ordered category (as we did for `ProductSize` earlier), in which case the ordering you chose is used. We can see the mapping by looking at the `classes` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#na#', 'Compact', 'Large', 'Large / Medium', 'Medium', 'Mini', 'Small']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.classes[\"ProductSize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it takes a minute or so to process the data to get to this point, we should save it - that way in the future we can continue our work from here without rerunning the previous steps. \n",
    "\n",
    "fastai provides a `save` method that uses Python's *pickle* system to save nearly any Python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(path/'to.pkl', to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read this back later, you would type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = load_pickle(path/'to.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        SalesID  SalePrice  MachineID  ModelID  ...  saleIs_year_start  \\\n",
       "0       1139246  11.097410     999089     3157  ...                  1   \n",
       "1       1139248  10.950807     117657       77  ...                  1   \n",
       "2       1139249   9.210340     434808     7009  ...                  1   \n",
       "3       1139251  10.558414    1026470      332  ...                  1   \n",
       "4       1139253   9.305651    1057373    17311  ...                  1   \n",
       "...         ...        ...        ...      ...  ...                ...   \n",
       "401120  6333336   9.259130    1840702    21439  ...                  1   \n",
       "401121  6333337   9.305651    1830472    21439  ...                  1   \n",
       "401122  6333338   9.350102    1887659    21439  ...                  1   \n",
       "401123  6333341   9.104980    1903570    21435  ...                  1   \n",
       "401124  6333342   8.955448    1926965    21435  ...                  1   \n",
       "\n",
       "         saleElapsed  auctioneerID_na  MachineHoursCurrentMeter_na  \n",
       "0       1.163635e+09                1                            1  \n",
       "1       1.080259e+09                1                            1  \n",
       "2       1.077754e+09                1                            1  \n",
       "3       1.305763e+09                1                            1  \n",
       "4       1.248307e+09                1                            1  \n",
       "...              ...              ...                          ...  \n",
       "401120  1.320192e+09                1                            2  \n",
       "401121  1.320192e+09                1                            2  \n",
       "401122  1.320192e+09                1                            2  \n",
       "401123  1.319501e+09                1                            2  \n",
       "401124  1.319501e+09                1                            2  \n",
       "\n",
       "[412698 rows x 67 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e90582675576e512e92beafc5221de7144a1dea5f49197901594ad708e1d2ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
